{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38718f0-5d86-4579-b38d-3fb17a990dd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141c10a-0807-4101-a94c-cb581f7aee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as f\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import optim\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AutoModelForSequenceClassification, AutoTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13e3bd-48ae-4d21-9db6-c99a99893a75",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get the data and remove the duplicates and NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcbf01-708d-4363-aac8-112d9862bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls | grep csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cdea4-1b9d-4537-9360-88dda9085bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe146901-9a15-44f2-be62-db689a9bc5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fddaef2-cfd7-46f4-a7a8-5214bd515249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(row): \n",
    "    return str(row['text']) + ' ' + str(row['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f1989-7e25-47a3-98c9-5764b5a43634",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_title'] = train_df.apply(combine_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71119e4-8d06-47f4-a78e-00742323106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=['text_title'], inplace=True)\n",
    "train_df.drop_duplicates(subset='text_title', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050fc89b-5f43-42d0-8862-6db8d08ed493",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88592e9d-9319-4297-b8d9-1954917915f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['title', 'author', 'text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a896a36-be3a-4d7e-be67-04deebb5a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d617813-4ebc-4549-b8ad-fcb60eb3e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: 'real', \n",
    "    1: 'fake'\n",
    "}\n",
    "label2id = {val: key for key, val in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479cea95-974a-4e60-aabb-67248827ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=train_df, x='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c233a-f6cd-40cb-8be9-8ee8cdc5ab70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Number of Characters in the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d22741-1b55-45fd-ba6c-829e19044918",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(15, 8), ncols=2)\n",
    "text_lens = train_df[train_df['label'] == 1]['text_title'].apply(lambda x: len(x.split()))\n",
    "sns.histplot(text_lens, ax=axs[0])\n",
    "axs[0].set_title('Number of characters in the news that are labeled as fake')\n",
    "text_lens = train_df[train_df['label'] == 0]['text_title'].apply(lambda x: len(x.split()))\n",
    "sns.histplot(text_lens, ax=axs[1])\n",
    "axs[1].set_title('Number of characters in the news that are labeled as real')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664949bf-461d-4e44-b604-4697d65a8ed4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846eeaa-54fe-438c-83d2-d7e92a7ccc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_text = ' '.join(train_df[train_df['label'] == 1]['text_title'].tolist())\n",
    "wc = WordCloud()\n",
    "wc_obj = wc.generate(fake_text)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wc_obj)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70186da1-64c3-4e09-abd4-68be677d21e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text = ' '.join(train_df[train_df['label'] == 0]['text_title'].tolist())\n",
    "wc = WordCloud()\n",
    "wc_obj = wc.generate(real_text)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(wc_obj)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0d361-6dbf-497d-a548-e9e559e75f20",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09530d-c751-4fe2-bfd4-dc99bf15abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text: str) -> str:\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text) # removing punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2bd3ed-8848-4652-bbe4-769448a1c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text_title'].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ff2de-256c-4903-a112-0c0fb42312d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split = train_test_split(train_df, stratify=train_df['label'], test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8adf0-3e83-49cb-88aa-f5385232b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, tokenizer: BertTokenizer, split):\n",
    "        self.texts = split['text_title'].tolist()\n",
    "        self.labels = split['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        encoded_text = tokenizer(\n",
    "            text = text, \n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        return encoded_text['input_ids'], encoded_text['attention_mask'], self.labels[index]\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699f510-d8fa-4e37-ab00-431a5986b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "train_ds = NewsDataset(tokenizer, train_split)\n",
    "val_ds = NewsDataset(tokenizer, val_split)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989a0bc-1cd6-4a54-adbd-640f520bc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_ids, attention_mask, labels in train_dl:\n",
    "    print(input_ids.shape, attention_mask.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de722d-ed77-408c-9758-ba7e60190514",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d314cb-40f0-4850-a7bc-2b087800ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device('cuda') if th.cuda.is_available() else th.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e799c-d5fb-4578-9adc-128a5d9c61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeNewsClassifier, self).__init__()\n",
    "        \n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        \n",
    "        last_hidden_state_cls = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8834f92-4d22-47b8-bf2a-69da9c245d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model = FakeNewsClassifier()\n",
    "for input_ids, attention_mask, labels in train_dl:\n",
    "    print(input_ids.shape, attention_mask.shape, labels.shape)\n",
    "    input_ids = input_ids.squeeze(1)\n",
    "    attention_mask = attention_mask.squeeze(1)\n",
    "    logits = sample_model(input_ids, attention_mask)\n",
    "    print(logits)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad970e9d-5c53-40fb-90c7-048e498e1bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, save_path, valid_loss):\n",
    "    if save_path == None:\n",
    "        print('Path to save the checkpoint is not valid!')\n",
    "        return\n",
    "\n",
    "    state_dict = {\n",
    "        'model_state_dict': model.state_dict, \n",
    "        'valid_loss': valid_loss\n",
    "    }\n",
    "    \n",
    "    th.save(state_dict, save_path)\n",
    "    print('Model saved to ==> {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1197abd1-ea18-42d0-a0cf-38dd0fbe1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = th.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26602aeb-625c-4132-a06e-84bab243b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, epochs=5, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "\n",
    "    val_accuracy_list = []\n",
    "    train_accuracy_list = []\n",
    "    val_loss_list = []\n",
    "    train_loss_list = []\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        train_epoch_accuracy_list = []\n",
    "        train_epoch_loss_list = []\n",
    "        \n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids = b_input_ids.squeeze(1)\n",
    "            b_attn_mask = b_attn_mask.squeeze(1)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "            accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "            train_epoch_accuracy_list.append(accuracy)\n",
    "            train_epoch_loss_list.append(loss.item())\n",
    "\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        train_accuracy_list.append(np.mean(train_epoch_accuracy_list))\n",
    "        train_loss_list.append(np.mean(train_epoch_loss_list))\n",
    "\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            val_loss_list.append(val_loss)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return train_accuracy_list, train_loss_list, val_accuracy_list, val_loss_list\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids = b_input_ids.squeeze(1)\n",
    "        b_attn_mask = b_attn_mask.squeeze(1)\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b589d1f-48fd-4728-bb57-413942078ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182d9ff-c82f-4ff9-b514-d737f52d0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=5):\n",
    "    model = FakeNewsClassifier()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "\n",
    "    total_steps = len(train_dl) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd740ebd-8dcb-4ef7-93df-da4115495795",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "model, optimizer, scheduler = initialize_model(epochs=5)\n",
    "train_accuracy_list, train_loss_list, val_accuracy_list, val_loss_list = train(model, train_dl, val_dl, epochs=5, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d1ac07-16aa-4d8a-88ab-50095e757209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-2.0]",
   "language": "python",
   "name": "conda-env-tf-2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
