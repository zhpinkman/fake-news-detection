{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhpinkman/fake-news-detection/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "b38718f0-5d86-4579-b38d-3fb17a990dd9"
      },
      "source": [
        "### Import the required libraries"
      ],
      "id": "b38718f0-5d86-4579-b38d-3fb17a990dd9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwsDRlPN44_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd127acd-d113-40a1-d3b1-7e81835a27a0"
      },
      "source": [
        "!pip install transformers"
      ],
      "id": "vwsDRlPN44_-",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQOBvOUx5R9r"
      },
      "source": [
        "cp kaggle.json /root/.kaggle/"
      ],
      "id": "yQOBvOUx5R9r",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYuWcM5q5eHe"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "id": "SYuWcM5q5eHe",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpIjrwHP5A6G",
        "outputId": "23098e41-b5c7-4c5c-856b-a211a617a320"
      },
      "source": [
        "!kaggle kernels output mastmustu/97-fake-news-detection-using-bert-pytorch -p ."
      ],
      "id": "OpIjrwHP5A6G",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/cli.py\", line 51, in main\n",
            "    out = args.func(**command_args)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 2069, in kernels_output_cli\n",
            "    self.kernels_output(kernel, path, force, quiet)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 2039, in kernels_output\n",
            "    os.makedirs(os.path.split(outfile)[0], exist_ok=True)\n",
            "TypeError: makedirs() got an unexpected keyword argument 'exist_ok'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1141c10a-0807-4101-a94c-cb581f7aee1e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch as th\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as f\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import optim\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AutoModelForSequenceClassification, AutoTokenizer, BertModel"
      ],
      "id": "1141c10a-0807-4101-a94c-cb581f7aee1e",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "6f13e3bd-48ae-4d21-9db6-c99a99893a75"
      },
      "source": [
        "### Get the data and remove the duplicates and NaNs"
      ],
      "id": "6f13e3bd-48ae-4d21-9db6-c99a99893a75"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adfcbf01-708d-4363-aac8-112d9862bb04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a9e1eb-37bf-48ad-8556-18d3f851ad92"
      },
      "source": [
        "ls | grep csv"
      ],
      "id": "adfcbf01-708d-4363-aac8-112d9862bb04",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e83cdea4-1b9d-4537-9360-88dda9085bfa"
      },
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "# test_df = pd.read_csv('test.csv')"
      ],
      "id": "e83cdea4-1b9d-4537-9360-88dda9085bfa",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe146901-9a15-44f2-be62-db689a9bc5c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a74d678-0ac6-4b13-c6f6-0d8ad7bc4bc8"
      },
      "source": [
        "train_df.info()"
      ],
      "id": "fe146901-9a15-44f2-be62-db689a9bc5c1",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20800 entries, 0 to 20799\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      20800 non-null  int64 \n",
            " 1   title   20242 non-null  object\n",
            " 2   author  18843 non-null  object\n",
            " 3   text    20761 non-null  object\n",
            " 4   label   20800 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 812.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fddaef2-cfd7-46f4-a7a8-5214bd515249"
      },
      "source": [
        "def combine_columns(row): \n",
        "    return str(row['text']) + ' ' + str(row['title'])"
      ],
      "id": "7fddaef2-cfd7-46f4-a7a8-5214bd515249",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "354f1989-7e25-47a3-98c9-5764b5a43634"
      },
      "source": [
        "train_df['text_title'] = train_df.apply(combine_columns, axis=1)"
      ],
      "id": "354f1989-7e25-47a3-98c9-5764b5a43634",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f71119e4-8d06-47f4-a78e-00742323106c"
      },
      "source": [
        "train_df.dropna(subset=['text_title'], inplace=True)\n",
        "train_df.drop_duplicates(subset='text_title', inplace=True)"
      ],
      "id": "f71119e4-8d06-47f4-a78e-00742323106c",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "050fc89b-5f43-42d0-8862-6db8d08ed493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee3f531-2193-4312-d5cf-4060abf84fa2"
      },
      "source": [
        "train_df.info()"
      ],
      "id": "050fc89b-5f43-42d0-8862-6db8d08ed493",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 20690 entries, 0 to 20799\n",
            "Data columns (total 6 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   id          20690 non-null  int64 \n",
            " 1   title       20172 non-null  object\n",
            " 2   author      18758 non-null  object\n",
            " 3   text        20651 non-null  object\n",
            " 4   label       20690 non-null  int64 \n",
            " 5   text_title  20690 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 1.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88592e9d-9319-4297-b8d9-1954917915f3"
      },
      "source": [
        "train_df.drop(columns=['title', 'author', 'text'], inplace=True)"
      ],
      "id": "88592e9d-9319-4297-b8d9-1954917915f3",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a896a36-be3a-4d7e-be67-04deebb5a040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "138e8ed8-8e8d-421c-96d0-5c5fb8ced74b"
      },
      "source": [
        "train_df.head(3)"
      ],
      "id": "3a896a36-be3a-4d7e-be67-04deebb5a040",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>text_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label                                         text_title\n",
              "0   0      1  House Dem Aide: We Didn’t Even See Comey’s Let...\n",
              "1   1      0  Ever get the feeling your life circles the rou...\n",
              "2   2      1  Why the Truth Might Get You Fired October 29, ..."
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d617813-4ebc-4549-b8ad-fcb60eb3e8a4"
      },
      "source": [
        "id2label = {\n",
        "    0: 'real', \n",
        "    1: 'fake'\n",
        "}\n",
        "label2id = {val: key for key, val in id2label.items()}"
      ],
      "id": "1d617813-4ebc-4549-b8ad-fcb60eb3e8a4",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "479cea95-974a-4e60-aabb-67248827ff88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "b35c6c6d-e18c-4e45-f40b-37ab60a5e912"
      },
      "source": [
        "sns.countplot(data=train_df, x='label')"
      ],
      "id": "479cea95-974a-4e60-aabb-67248827ff88",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbb304c2c10>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "8e2c233a-f6cd-40cb-8be9-8ee8cdc5ab70"
      },
      "source": [
        "### Number of Characters in the Text"
      ],
      "id": "8e2c233a-f6cd-40cb-8be9-8ee8cdc5ab70"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5d22741-1b55-45fd-ba6c-829e19044918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "outputId": "f484635c-4a31-430f-d99b-bfb6f121086f"
      },
      "source": [
        "fig, axs = plt.subplots(figsize=(15, 8), ncols=2)\n",
        "text_lens = train_df[train_df['label'] == 1]['text_title'].apply(lambda x: len(x.split()))\n",
        "sns.histplot(text_lens, ax=axs[0])\n",
        "axs[0].set_title('Number of characters in the news that are labeled as fake')\n",
        "text_lens = train_df[train_df['label'] == 0]['text_title'].apply(lambda x: len(x.split()))\n",
        "sns.histplot(text_lens, ax=axs[1])\n",
        "axs[1].set_title('Number of characters in the news that are labeled as real')"
      ],
      "id": "a5d22741-1b55-45fd-ba6c-829e19044918",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Number of characters in the news that are labeled as real')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "664949bf-461d-4e44-b604-4697d65a8ed4"
      },
      "source": [
        "### Word Cloud"
      ],
      "id": "664949bf-461d-4e44-b604-4697d65a8ed4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9846eeaa-54fe-438c-83d2-d7e92a7ccc5f"
      },
      "source": [
        "# fake_text = ' '.join(train_df[train_df['label'] == 1]['text_title'].tolist())\n",
        "# wc = WordCloud()\n",
        "# wc_obj = wc.generate(fake_text)\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# plt.imshow(wc_obj)\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ],
      "id": "9846eeaa-54fe-438c-83d2-d7e92a7ccc5f",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70186da1-64c3-4e09-abd4-68be677d21e8"
      },
      "source": [
        "# real_text = ' '.join(train_df[train_df['label'] == 0]['text_title'].tolist())\n",
        "# wc = WordCloud()\n",
        "# wc_obj = wc.generate(real_text)\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# plt.imshow(wc_obj)\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ],
      "id": "70186da1-64c3-4e09-abd4-68be677d21e8",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59f0d361-6dbf-497d-a548-e9e559e75f20"
      },
      "source": [
        "### Loading the dataset"
      ],
      "id": "59f0d361-6dbf-497d-a548-e9e559e75f20"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e09530d-c751-4fe2-bfd4-dc99bf15abb5"
      },
      "source": [
        "def text_cleaning(text: str) -> str:\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text) # removing punctuation\n",
        "    return text"
      ],
      "id": "6e09530d-c751-4fe2-bfd4-dc99bf15abb5",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f2bd3ed-8848-4652-bbe4-769448a1c5be"
      },
      "source": [
        "train_df['text'] = train_df['text_title'].apply(text_cleaning)"
      ],
      "id": "2f2bd3ed-8848-4652-bbe4-769448a1c5be",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c2ff2de-256c-4903-a112-0c0fb42312d0"
      },
      "source": [
        "train_split, val_split = train_test_split(train_df, stratify=train_df['label'], test_size=.2)"
      ],
      "id": "1c2ff2de-256c-4903-a112-0c0fb42312d0",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eb8adf0-3e83-49cb-88aa-f5385232b9db"
      },
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, tokenizer: BertTokenizer, split):\n",
        "        self.texts = split['text_title'].tolist()\n",
        "        self.labels = split['label'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        encoded_text = tokenizer(\n",
        "            text = text, \n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        return encoded_text['input_ids'], encoded_text['attention_mask'], self.labels[index]\n",
        "    def __len__(self):\n",
        "        return len(self.texts)"
      ],
      "id": "7eb8adf0-3e83-49cb-88aa-f5385232b9db",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d699f510-d8fa-4e37-ab00-431a5986b849"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "train_ds = NewsDataset(tokenizer, train_split)\n",
        "val_ds = NewsDataset(tokenizer, val_split)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=8, shuffle=False)"
      ],
      "id": "d699f510-d8fa-4e37-ab00-431a5986b849",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9989a0bc-1cd6-4a54-adbd-640f520bc660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d77fb3-447e-4cd1-b7e9-90a1d0f97bb5"
      },
      "source": [
        "for input_ids, attention_mask, labels in train_dl:\n",
        "    print(input_ids.shape, attention_mask.shape, labels.shape)\n",
        "    break"
      ],
      "id": "9989a0bc-1cd6-4a54-adbd-640f520bc660",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1, 512]) torch.Size([8, 1, 512]) torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85de722d-ed77-408c-9758-ba7e60190514"
      },
      "source": [
        "### Model"
      ],
      "id": "85de722d-ed77-408c-9758-ba7e60190514"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7d314cb-40f0-4850-a7bc-2b087800ced0"
      },
      "source": [
        "device = th.device('cuda') if th.cuda.is_available() else th.device('cpu')"
      ],
      "id": "b7d314cb-40f0-4850-a7bc-2b087800ced0",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a6e799c-d5fb-4578-9adc-128a5d9c61ab"
      },
      "source": [
        "class FakeNewsClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FakeNewsClassifier, self).__init__()\n",
        "        \n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        \n",
        "        last_hidden_state_cls = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "        \n",
        "        return logits"
      ],
      "id": "7a6e799c-d5fb-4578-9adc-128a5d9c61ab",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8834f92-4d22-47b8-bf2a-69da9c245d00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3088c98f-85c3-46a8-cfa8-4dd7171414d2"
      },
      "source": [
        "sample_model = FakeNewsClassifier()\n",
        "for input_ids, attention_mask, labels in train_dl:\n",
        "    print(input_ids.shape, attention_mask.shape, labels.shape)\n",
        "    input_ids = input_ids.squeeze(1)\n",
        "    attention_mask = attention_mask.squeeze(1)\n",
        "    logits = sample_model(input_ids, attention_mask)\n",
        "    print(logits)\n",
        "    break"
      ],
      "id": "f8834f92-4d22-47b8-bf2a-69da9c245d00",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1, 512]) torch.Size([8, 1, 512]) torch.Size([8])\n",
            "tensor([[-0.2139, -0.1672],\n",
            "        [-0.0693, -0.3407],\n",
            "        [-0.1519,  0.0249],\n",
            "        [-0.1978, -0.1988],\n",
            "        [-0.1204, -0.2086],\n",
            "        [-0.2722, -0.0675],\n",
            "        [-0.2052, -0.0561],\n",
            "        [-0.3250, -0.1901]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad970e9d-5c53-40fb-90c7-048e498e1bba"
      },
      "source": [
        "def save_checkpoint(model, save_path, valid_loss):\n",
        "    if save_path == None:\n",
        "        print('Path to save the checkpoint is not valid!')\n",
        "        return\n",
        "\n",
        "    state_dict = {\n",
        "        'model_state_dict': model.state_dict, \n",
        "        'valid_loss': valid_loss\n",
        "    }\n",
        "    \n",
        "    th.save(state_dict, save_path)\n",
        "    print('Model saved to ==> {}'.format(save_path))"
      ],
      "id": "ad970e9d-5c53-40fb-90c7-048e498e1bba",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1197abd1-ea18-42d0-a0cf-38dd0fbe1ca2"
      },
      "source": [
        "def load_checkpoint(load_path, model):\n",
        "    \n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = th.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']"
      ],
      "id": "1197abd1-ea18-42d0-a0cf-38dd0fbe1ca2",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zotPAopaZToC"
      },
      "source": [
        "### Training the Model"
      ],
      "id": "zotPAopaZToC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26602aeb-625c-4132-a06e-84bab243b2a0"
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, epochs=5, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "\n",
        "    val_accuracy_list = []\n",
        "    train_accuracy_list = []\n",
        "    val_loss_list = []\n",
        "    train_loss_list = []\n",
        "\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "\n",
        "        train_epoch_accuracy_list = []\n",
        "        train_epoch_loss_list = []\n",
        "        \n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        \n",
        "        model.train()\n",
        "\n",
        "        \n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "            b_input_ids = b_input_ids.squeeze(1)\n",
        "            b_attn_mask = b_attn_mask.squeeze(1)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "            accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "            train_epoch_accuracy_list.append(accuracy)\n",
        "            train_epoch_loss_list.append(loss.item())\n",
        "\n",
        "\n",
        "            \n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            \n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        train_accuracy_list.append(np.mean(train_epoch_accuracy_list))\n",
        "        train_loss_list.append(np.mean(train_epoch_loss_list))\n",
        "\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        if evaluation == True:\n",
        "            \n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "            val_accuracy_list.append(val_accuracy)\n",
        "            val_loss_list.append(val_loss)\n",
        "\n",
        "            \n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "    return train_accuracy_list, train_loss_list, val_accuracy_list, val_loss_list\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    \n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    \n",
        "    for batch in val_dataloader:\n",
        "    \n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids = b_input_ids.squeeze(1)\n",
        "        b_attn_mask = b_attn_mask.squeeze(1)\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "    \n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "    \n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "    \n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    \n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "id": "26602aeb-625c-4132-a06e-84bab243b2a0",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b589d1f-48fd-4728-bb57-413942078ac2"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "id": "0b589d1f-48fd-4728-bb57-413942078ac2",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4182d9ff-c82f-4ff9-b514-d737f52d0ccb"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=5):\n",
        "    model = FakeNewsClassifier()\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
        "\n",
        "\n",
        "    total_steps = len(train_dl) * epochs\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "    return model, optimizer, scheduler"
      ],
      "id": "4182d9ff-c82f-4ff9-b514-d737f52d0ccb",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd740ebd-8dcb-4ef7-93df-da4115495795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006c1a6b-5c7f-4be6-8c67-3ca70285ed48"
      },
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "model, optimizer, scheduler = initialize_model(epochs=5)\n",
        "train_accuracy_list, train_loss_list, val_accuracy_list, val_loss_list = train(model, train_dl, val_dl, epochs=5, evaluation=True)"
      ],
      "id": "cd740ebd-8dcb-4ef7-93df-da4115495795",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.669046   |     -      |     -     |   32.26  \n",
            "   1    |   40    |   0.292016   |     -      |     -     |   30.73  \n",
            "   1    |   60    |   0.289912   |     -      |     -     |   31.98  \n",
            "   1    |   80    |   0.402604   |     -      |     -     |   31.52  \n",
            "   1    |   100   |   0.359794   |     -      |     -     |   31.52  \n",
            "   1    |   120   |   0.210973   |     -      |     -     |   31.56  \n",
            "   1    |   140   |   0.157363   |     -      |     -     |   31.66  \n",
            "   1    |   160   |   0.123079   |     -      |     -     |   31.60  \n",
            "   1    |   180   |   0.130971   |     -      |     -     |   31.83  \n",
            "   1    |   200   |   0.103240   |     -      |     -     |   31.48  \n",
            "   1    |   220   |   0.190731   |     -      |     -     |   32.20  \n",
            "   1    |   240   |   0.238128   |     -      |     -     |   31.30  \n",
            "   1    |   260   |   0.112035   |     -      |     -     |   31.78  \n",
            "   1    |   280   |   0.094739   |     -      |     -     |   31.26  \n",
            "   1    |   300   |   0.065900   |     -      |     -     |   31.46  \n",
            "   1    |   320   |   0.156464   |     -      |     -     |   31.87  \n",
            "   1    |   340   |   0.102077   |     -      |     -     |   31.62  \n",
            "   1    |   360   |   0.143258   |     -      |     -     |   31.48  \n",
            "   1    |   380   |   0.219755   |     -      |     -     |   32.04  \n",
            "   1    |   400   |   0.269354   |     -      |     -     |   31.80  \n",
            "   1    |   420   |   0.117013   |     -      |     -     |   31.62  \n",
            "   1    |   440   |   0.130418   |     -      |     -     |   31.07  \n",
            "   1    |   460   |   0.148587   |     -      |     -     |   31.26  \n",
            "   1    |   480   |   0.073862   |     -      |     -     |   31.34  \n",
            "   1    |   500   |   0.144942   |     -      |     -     |   31.55  \n",
            "   1    |   520   |   0.155710   |     -      |     -     |   31.28  \n",
            "   1    |   540   |   0.179809   |     -      |     -     |   31.16  \n",
            "   1    |   560   |   0.103910   |     -      |     -     |   31.39  \n",
            "   1    |   580   |   0.152021   |     -      |     -     |   31.18  \n",
            "   1    |   600   |   0.110262   |     -      |     -     |   31.01  \n",
            "   1    |   620   |   0.123103   |     -      |     -     |   31.39  \n",
            "   1    |   640   |   0.166680   |     -      |     -     |   31.32  \n",
            "   1    |   660   |   0.166523   |     -      |     -     |   31.82  \n",
            "   1    |   680   |   0.127573   |     -      |     -     |   31.55  \n",
            "   1    |   700   |   0.125157   |     -      |     -     |   31.40  \n",
            "   1    |   720   |   0.020215   |     -      |     -     |   32.46  \n",
            "   1    |   740   |   0.118343   |     -      |     -     |   31.21  \n",
            "   1    |   760   |   0.078966   |     -      |     -     |   31.48  \n",
            "   1    |   780   |   0.092696   |     -      |     -     |   31.31  \n",
            "   1    |   800   |   0.338876   |     -      |     -     |   31.28  \n",
            "   1    |   820   |   0.084068   |     -      |     -     |   31.47  \n",
            "   1    |   840   |   0.114608   |     -      |     -     |   31.85  \n",
            "   1    |   860   |   0.145026   |     -      |     -     |   31.67  \n",
            "   1    |   880   |   0.121533   |     -      |     -     |   31.77  \n",
            "   1    |   900   |   0.115793   |     -      |     -     |   31.41  \n",
            "   1    |   920   |   0.109602   |     -      |     -     |   31.58  \n",
            "   1    |   940   |   0.089268   |     -      |     -     |   31.57  \n",
            "   1    |   960   |   0.003010   |     -      |     -     |   32.15  \n",
            "   1    |   980   |   0.155180   |     -      |     -     |   31.03  \n",
            "   1    |  1000   |   0.114671   |     -      |     -     |   32.09  \n",
            "   1    |  1020   |   0.091892   |     -      |     -     |   31.24  \n",
            "   1    |  1040   |   0.175759   |     -      |     -     |   31.46  \n",
            "   1    |  1060   |   0.004115   |     -      |     -     |   31.12  \n",
            "   1    |  1080   |   0.081661   |     -      |     -     |   31.59  \n",
            "   1    |  1100   |   0.368662   |     -      |     -     |   31.61  \n",
            "   1    |  1120   |   0.053556   |     -      |     -     |   31.42  \n",
            "   1    |  1140   |   0.143368   |     -      |     -     |   32.00  \n",
            "   1    |  1160   |   0.003246   |     -      |     -     |   31.31  \n",
            "   1    |  1180   |   0.006846   |     -      |     -     |   31.42  \n",
            "   1    |  1200   |   0.122326   |     -      |     -     |   31.65  \n",
            "   1    |  1220   |   0.053507   |     -      |     -     |   31.60  \n",
            "   1    |  1240   |   0.162829   |     -      |     -     |   31.62  \n",
            "   1    |  1260   |   0.097217   |     -      |     -     |   31.54  \n",
            "   1    |  1280   |   0.003659   |     -      |     -     |   31.30  \n",
            "   1    |  1300   |   0.062662   |     -      |     -     |   31.55  \n",
            "   1    |  1320   |   0.043433   |     -      |     -     |   32.41  \n",
            "   1    |  1340   |   0.048460   |     -      |     -     |   31.83  \n",
            "   1    |  1360   |   0.055810   |     -      |     -     |   31.01  \n",
            "   1    |  1380   |   0.069239   |     -      |     -     |   31.44  \n",
            "   1    |  1400   |   0.018586   |     -      |     -     |   31.41  \n",
            "   1    |  1420   |   0.001347   |     -      |     -     |   31.23  \n",
            "   1    |  1440   |   0.002304   |     -      |     -     |   31.66  \n",
            "   1    |  1460   |   0.022978   |     -      |     -     |   31.55  \n",
            "   1    |  1480   |   0.040636   |     -      |     -     |   31.54  \n",
            "   1    |  1500   |   0.109810   |     -      |     -     |   31.44  \n",
            "   1    |  1520   |   0.001572   |     -      |     -     |   31.58  \n",
            "   1    |  1540   |   0.042579   |     -      |     -     |   30.93  \n",
            "   1    |  1560   |   0.083322   |     -      |     -     |   31.65  \n",
            "   1    |  1580   |   0.053066   |     -      |     -     |   31.80  \n",
            "   1    |  1600   |   0.135743   |     -      |     -     |   32.30  \n",
            "   1    |  1620   |   0.074373   |     -      |     -     |   31.38  \n",
            "   1    |  1640   |   0.029461   |     -      |     -     |   31.40  \n",
            "   1    |  1660   |   0.090539   |     -      |     -     |   31.45  \n",
            "   1    |  1680   |   0.064164   |     -      |     -     |   31.54  \n",
            "   1    |  1700   |   0.021435   |     -      |     -     |   30.95  \n",
            "   1    |  1720   |   0.001573   |     -      |     -     |   31.47  \n",
            "   1    |  1740   |   0.109213   |     -      |     -     |   31.63  \n",
            "   1    |  1760   |   0.030925   |     -      |     -     |   31.48  \n",
            "   1    |  1780   |   0.047348   |     -      |     -     |   31.47  \n",
            "   1    |  1800   |   0.001370   |     -      |     -     |   31.48  \n",
            "   1    |  1820   |   0.049284   |     -      |     -     |   31.59  \n",
            "   1    |  1840   |   0.076906   |     -      |     -     |   31.20  \n",
            "   1    |  1860   |   0.001350   |     -      |     -     |   31.42  \n",
            "   1    |  1880   |   0.197195   |     -      |     -     |   31.78  \n",
            "   1    |  1900   |   0.064821   |     -      |     -     |   31.70  \n",
            "   1    |  1920   |   0.081815   |     -      |     -     |   31.28  \n",
            "   1    |  1940   |   0.014030   |     -      |     -     |   32.03  \n",
            "   1    |  1960   |   0.096791   |     -      |     -     |   31.38  \n",
            "   1    |  1980   |   0.018363   |     -      |     -     |   31.40  \n",
            "   1    |  2000   |   0.120331   |     -      |     -     |   31.82  \n",
            "   1    |  2020   |   0.037552   |     -      |     -     |   31.81  \n",
            "   1    |  2040   |   0.056214   |     -      |     -     |   31.25  \n",
            "   1    |  2060   |   0.053708   |     -      |     -     |   31.67  \n",
            "   1    |  2068   |   0.000988   |     -      |     -     |   12.63  \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.112051   |  0.078159  |   98.72   |  3600.11 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.035915   |     -      |     -     |   33.01  \n",
            "   2    |   40    |   0.044120   |     -      |     -     |   31.44  \n",
            "   2    |   60    |   0.104703   |     -      |     -     |   31.60  \n",
            "   2    |   80    |   0.001227   |     -      |     -     |   31.48  \n",
            "   2    |   100   |   0.000502   |     -      |     -     |   31.33  \n",
            "   2    |   120   |   0.000705   |     -      |     -     |   31.67  \n",
            "   2    |   140   |   0.080830   |     -      |     -     |   31.70  \n",
            "   2    |   160   |   0.015747   |     -      |     -     |   31.58  \n",
            "   2    |   180   |   0.037367   |     -      |     -     |   31.64  \n",
            "   2    |   200   |   0.060700   |     -      |     -     |   32.00  \n",
            "   2    |   220   |   0.049100   |     -      |     -     |   31.08  \n",
            "   2    |   240   |   0.000778   |     -      |     -     |   31.83  \n",
            "   2    |   260   |   0.002487   |     -      |     -     |   31.50  \n",
            "   2    |   280   |   0.000350   |     -      |     -     |   32.36  \n",
            "   2    |   300   |   0.000308   |     -      |     -     |   31.78  \n",
            "   2    |   320   |   0.000261   |     -      |     -     |   31.37  \n",
            "   2    |   340   |   0.000391   |     -      |     -     |   31.73  \n",
            "   2    |   360   |   0.000324   |     -      |     -     |   31.86  \n",
            "   2    |   380   |   0.000534   |     -      |     -     |   31.55  \n",
            "   2    |   400   |   0.000221   |     -      |     -     |   31.62  \n",
            "   2    |   420   |   0.046704   |     -      |     -     |   31.58  \n",
            "   2    |   440   |   0.047327   |     -      |     -     |   31.74  \n",
            "   2    |   460   |   0.020467   |     -      |     -     |   31.35  \n",
            "   2    |   480   |   0.011517   |     -      |     -     |   31.67  \n",
            "   2    |   500   |   0.000558   |     -      |     -     |   31.85  \n",
            "   2    |   520   |   0.000126   |     -      |     -     |   31.68  \n",
            "   2    |   540   |   0.123771   |     -      |     -     |   31.94  \n",
            "   2    |   560   |   0.035208   |     -      |     -     |   30.99  \n",
            "   2    |   580   |   0.000325   |     -      |     -     |   31.52  \n",
            "   2    |   600   |   0.002515   |     -      |     -     |   31.29  \n",
            "   2    |   620   |   0.000323   |     -      |     -     |   31.54  \n",
            "   2    |   640   |   0.035478   |     -      |     -     |   32.60  \n",
            "   2    |   660   |   0.000635   |     -      |     -     |   31.36  \n",
            "   2    |   680   |   0.000481   |     -      |     -     |   31.13  \n",
            "   2    |   700   |   0.000145   |     -      |     -     |   31.66  \n",
            "   2    |   720   |   0.065073   |     -      |     -     |   31.66  \n",
            "   2    |   740   |   0.000354   |     -      |     -     |   31.46  \n",
            "   2    |   760   |   0.016824   |     -      |     -     |   31.96  \n",
            "   2    |   780   |   0.000946   |     -      |     -     |   31.55  \n",
            "   2    |   800   |   0.051602   |     -      |     -     |   31.73  \n",
            "   2    |   820   |   0.066861   |     -      |     -     |   31.35  \n",
            "   2    |   840   |   0.079720   |     -      |     -     |   31.78  \n",
            "   2    |   860   |   0.014800   |     -      |     -     |   31.44  \n",
            "   2    |   880   |   0.000613   |     -      |     -     |   31.54  \n",
            "   2    |   900   |   0.102019   |     -      |     -     |   31.34  \n",
            "   2    |   920   |   0.106055   |     -      |     -     |   31.25  \n",
            "   2    |   940   |   0.138394   |     -      |     -     |   31.64  \n",
            "   2    |   960   |   0.000583   |     -      |     -     |   31.95  \n",
            "   2    |   980   |   0.000565   |     -      |     -     |   31.28  \n",
            "   2    |  1000   |   0.000754   |     -      |     -     |   31.38  \n",
            "   2    |  1020   |   0.030695   |     -      |     -     |   32.02  \n",
            "   2    |  1040   |   0.000752   |     -      |     -     |   31.56  \n",
            "   2    |  1060   |   0.019535   |     -      |     -     |   31.93  \n",
            "   2    |  1080   |   0.000632   |     -      |     -     |   31.46  \n",
            "   2    |  1100   |   0.049844   |     -      |     -     |   31.20  \n",
            "   2    |  1120   |   0.000313   |     -      |     -     |   31.29  \n",
            "   2    |  1140   |   0.057445   |     -      |     -     |   31.20  \n",
            "   2    |  1160   |   0.000739   |     -      |     -     |   31.13  \n",
            "   2    |  1180   |   0.085580   |     -      |     -     |   31.25  \n",
            "   2    |  1200   |   0.000484   |     -      |     -     |   31.38  \n",
            "   2    |  1220   |   0.001165   |     -      |     -     |   31.31  \n",
            "   2    |  1240   |   0.061395   |     -      |     -     |   31.68  \n",
            "   2    |  1260   |   0.000453   |     -      |     -     |   31.23  \n",
            "   2    |  1280   |   0.000511   |     -      |     -     |   31.60  \n",
            "   2    |  1300   |   0.000481   |     -      |     -     |   31.30  \n",
            "   2    |  1320   |   0.000437   |     -      |     -     |   31.06  \n",
            "   2    |  1340   |   0.000472   |     -      |     -     |   31.42  \n",
            "   2    |  1360   |   0.000287   |     -      |     -     |   31.16  \n",
            "   2    |  1380   |   0.000249   |     -      |     -     |   31.31  \n",
            "   2    |  1400   |   0.071513   |     -      |     -     |   31.34  \n",
            "   2    |  1420   |   0.000642   |     -      |     -     |   31.57  \n",
            "   2    |  1440   |   0.047166   |     -      |     -     |   31.22  \n",
            "   2    |  1460   |   0.013730   |     -      |     -     |   31.22  \n",
            "   2    |  1480   |   0.029765   |     -      |     -     |   31.43  \n",
            "   2    |  1500   |   0.000440   |     -      |     -     |   31.28  \n",
            "   2    |  1520   |   0.082697   |     -      |     -     |   31.27  \n",
            "   2    |  1540   |   0.000349   |     -      |     -     |   31.60  \n",
            "   2    |  1560   |   0.004183   |     -      |     -     |   31.36  \n",
            "   2    |  1580   |   0.000205   |     -      |     -     |   31.67  \n",
            "   2    |  1600   |   0.047258   |     -      |     -     |   31.27  \n",
            "   2    |  1620   |   0.000420   |     -      |     -     |   31.37  \n",
            "   2    |  1640   |   0.070730   |     -      |     -     |   31.46  \n",
            "   2    |  1660   |   0.000365   |     -      |     -     |   31.67  \n",
            "   2    |  1680   |   0.000466   |     -      |     -     |   31.54  \n",
            "   2    |  1700   |   0.000412   |     -      |     -     |   31.42  \n",
            "   2    |  1720   |   0.069624   |     -      |     -     |   31.31  \n",
            "   2    |  1740   |   0.112391   |     -      |     -     |   31.73  \n",
            "   2    |  1760   |   0.001930   |     -      |     -     |   31.65  \n",
            "   2    |  1780   |   0.070219   |     -      |     -     |   31.02  \n",
            "   2    |  1800   |   0.116499   |     -      |     -     |   31.70  \n",
            "   2    |  1820   |   0.079601   |     -      |     -     |   31.26  \n",
            "   2    |  1840   |   0.000458   |     -      |     -     |   31.35  \n",
            "   2    |  1860   |   0.000581   |     -      |     -     |   31.27  \n",
            "   2    |  1880   |   0.000624   |     -      |     -     |   31.51  \n",
            "   2    |  1900   |   0.000270   |     -      |     -     |   31.17  \n",
            "   2    |  1920   |   0.070093   |     -      |     -     |   31.48  \n",
            "   2    |  1940   |   0.001910   |     -      |     -     |   31.83  \n",
            "   2    |  1960   |   0.000231   |     -      |     -     |   32.11  \n",
            "   2    |  1980   |   0.000238   |     -      |     -     |   31.33  \n",
            "   2    |  2000   |   0.096150   |     -      |     -     |   31.44  \n",
            "   2    |  2020   |   0.000432   |     -      |     -     |   31.60  \n",
            "   2    |  2040   |   0.000175   |     -      |     -     |   31.32  \n",
            "   2    |  2060   |   0.050915   |     -      |     -     |   31.29  \n",
            "   2    |  2068   |   0.000230   |     -      |     -     |   12.39  \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.026712   |  0.043712  |   99.42   |  3594.87 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   3    |   20    |   0.001089   |     -      |     -     |   33.14  \n",
            "   3    |   40    |   0.000701   |     -      |     -     |   31.06  \n",
            "   3    |   60    |   0.000500   |     -      |     -     |   31.49  \n",
            "   3    |   80    |   0.085905   |     -      |     -     |   31.24  \n",
            "   3    |   100   |   0.000887   |     -      |     -     |   31.69  \n",
            "   3    |   120   |   0.000677   |     -      |     -     |   31.14  \n",
            "   3    |   140   |   0.000181   |     -      |     -     |   30.99  \n",
            "   3    |   160   |   0.089400   |     -      |     -     |   31.11  \n",
            "   3    |   180   |   0.061953   |     -      |     -     |   31.19  \n",
            "   3    |   200   |   0.007431   |     -      |     -     |   31.28  \n",
            "   3    |   220   |   0.001731   |     -      |     -     |   31.69  \n",
            "   3    |   240   |   0.074308   |     -      |     -     |   31.24  \n",
            "   3    |   260   |   0.063900   |     -      |     -     |   31.58  \n",
            "   3    |   280   |   0.055318   |     -      |     -     |   31.28  \n",
            "   3    |   300   |   0.000379   |     -      |     -     |   31.20  \n",
            "   3    |   320   |   0.000111   |     -      |     -     |   31.76  \n",
            "   3    |   340   |   0.000220   |     -      |     -     |   31.88  \n",
            "   3    |   360   |   0.000455   |     -      |     -     |   31.72  \n",
            "   3    |   380   |   0.001650   |     -      |     -     |   31.85  \n",
            "   3    |   400   |   0.113614   |     -      |     -     |   31.44  \n",
            "   3    |   420   |   0.001061   |     -      |     -     |   31.98  \n",
            "   3    |   440   |   0.000946   |     -      |     -     |   31.50  \n",
            "   3    |   460   |   0.079257   |     -      |     -     |   30.93  \n",
            "   3    |   480   |   0.073897   |     -      |     -     |   31.45  \n",
            "   3    |   500   |   0.000780   |     -      |     -     |   31.64  \n",
            "   3    |   520   |   0.000500   |     -      |     -     |   31.51  \n",
            "   3    |   540   |   0.086828   |     -      |     -     |   31.23  \n",
            "   3    |   560   |   0.067935   |     -      |     -     |   31.51  \n",
            "   3    |   580   |   0.001374   |     -      |     -     |   31.68  \n",
            "   3    |   600   |   0.063983   |     -      |     -     |   31.74  \n",
            "   3    |   620   |   0.000273   |     -      |     -     |   31.82  \n",
            "   3    |   640   |   0.129039   |     -      |     -     |   31.34  \n",
            "   3    |   660   |   0.000636   |     -      |     -     |   31.58  \n",
            "   3    |   680   |   0.000284   |     -      |     -     |   31.80  \n",
            "   3    |   700   |   0.047422   |     -      |     -     |   31.65  \n",
            "   3    |   720   |   0.000544   |     -      |     -     |   32.02  \n",
            "   3    |   740   |   0.036249   |     -      |     -     |   31.13  \n",
            "   3    |   760   |   0.000623   |     -      |     -     |   32.05  \n",
            "   3    |   780   |   0.000353   |     -      |     -     |   31.43  \n",
            "   3    |   800   |   0.005535   |     -      |     -     |   31.71  \n",
            "   3    |   820   |   0.080340   |     -      |     -     |   32.10  \n",
            "   3    |   840   |   0.066966   |     -      |     -     |   31.53  \n",
            "   3    |   860   |   0.000234   |     -      |     -     |   31.89  \n",
            "   3    |   880   |   0.052848   |     -      |     -     |   31.96  \n",
            "   3    |   900   |   0.000762   |     -      |     -     |   31.38  \n",
            "   3    |   920   |   0.000378   |     -      |     -     |   31.23  \n",
            "   3    |   940   |   0.000340   |     -      |     -     |   31.50  \n",
            "   3    |   960   |   0.027582   |     -      |     -     |   31.50  \n",
            "   3    |   980   |   0.000327   |     -      |     -     |   31.59  \n",
            "   3    |  1000   |   0.000146   |     -      |     -     |   31.51  \n",
            "   3    |  1020   |   0.000244   |     -      |     -     |   31.29  \n",
            "   3    |  1040   |   0.000169   |     -      |     -     |   32.13  \n",
            "   3    |  1060   |   0.000062   |     -      |     -     |   31.56  \n",
            "   3    |  1080   |   0.000196   |     -      |     -     |   31.57  \n",
            "   3    |  1100   |   0.000107   |     -      |     -     |   31.50  \n",
            "   3    |  1120   |   0.057225   |     -      |     -     |   31.61  \n",
            "   3    |  1140   |   0.000150   |     -      |     -     |   31.31  \n",
            "   3    |  1160   |   0.000163   |     -      |     -     |   31.76  \n",
            "   3    |  1180   |   0.000289   |     -      |     -     |   31.51  \n",
            "   3    |  1200   |   0.000226   |     -      |     -     |   31.72  \n",
            "   3    |  1220   |   0.000097   |     -      |     -     |   31.69  \n",
            "   3    |  1240   |   0.071754   |     -      |     -     |   31.76  \n",
            "   3    |  1260   |   0.000303   |     -      |     -     |   31.44  \n",
            "   3    |  1280   |   0.000237   |     -      |     -     |   31.94  \n",
            "   3    |  1300   |   0.017903   |     -      |     -     |   31.31  \n",
            "   3    |  1320   |   0.002866   |     -      |     -     |   31.39  \n",
            "   3    |  1340   |   0.000418   |     -      |     -     |   31.60  \n",
            "   3    |  1360   |   0.000135   |     -      |     -     |   31.58  \n",
            "   3    |  1380   |   0.001853   |     -      |     -     |   31.92  \n",
            "   3    |  1400   |   0.083604   |     -      |     -     |   31.89  \n",
            "   3    |  1420   |   0.086453   |     -      |     -     |   31.91  \n",
            "   3    |  1440   |   0.000312   |     -      |     -     |   31.87  \n",
            "   3    |  1460   |   0.000216   |     -      |     -     |   31.68  \n",
            "   3    |  1480   |   0.000412   |     -      |     -     |   31.49  \n",
            "   3    |  1500   |   0.000232   |     -      |     -     |   31.37  \n",
            "   3    |  1520   |   0.000165   |     -      |     -     |   31.67  \n",
            "   3    |  1540   |   0.000302   |     -      |     -     |   32.07  \n",
            "   3    |  1560   |   0.000167   |     -      |     -     |   31.24  \n",
            "   3    |  1580   |   0.000538   |     -      |     -     |   31.59  \n",
            "   3    |  1600   |   0.000249   |     -      |     -     |   31.45  \n",
            "   3    |  1620   |   0.000344   |     -      |     -     |   31.10  \n",
            "   3    |  1640   |   0.000405   |     -      |     -     |   31.52  \n",
            "   3    |  1660   |   0.000169   |     -      |     -     |   31.83  \n",
            "   3    |  1680   |   0.000268   |     -      |     -     |   31.76  \n",
            "   3    |  1700   |   0.000222   |     -      |     -     |   31.62  \n",
            "   3    |  1720   |   0.000155   |     -      |     -     |   31.77  \n",
            "   3    |  1740   |   0.000119   |     -      |     -     |   31.27  \n",
            "   3    |  1760   |   0.000187   |     -      |     -     |   31.24  \n",
            "   3    |  1780   |   0.000907   |     -      |     -     |   32.26  \n",
            "   3    |  1800   |   0.060609   |     -      |     -     |   31.21  \n",
            "   3    |  1820   |   0.000563   |     -      |     -     |   31.77  \n",
            "   3    |  1840   |   0.049978   |     -      |     -     |   31.39  \n",
            "   3    |  1860   |   0.024686   |     -      |     -     |   31.54  \n",
            "   3    |  1880   |   0.000178   |     -      |     -     |   31.31  \n",
            "   3    |  1900   |   0.002106   |     -      |     -     |   31.51  \n",
            "   3    |  1920   |   0.000133   |     -      |     -     |   32.13  \n",
            "   3    |  1940   |   0.000201   |     -      |     -     |   31.57  \n",
            "   3    |  1960   |   0.021270   |     -      |     -     |   32.21  \n",
            "   3    |  1980   |   0.000393   |     -      |     -     |   31.46  \n",
            "   3    |  2000   |   0.000189   |     -      |     -     |   32.09  \n",
            "   3    |  2020   |   0.000116   |     -      |     -     |   31.61  \n",
            "   3    |  2040   |   0.060451   |     -      |     -     |   31.63  \n",
            "   3    |  2060   |   0.071979   |     -      |     -     |   32.09  \n",
            "   3    |  2068   |   0.000112   |     -      |     -     |   12.58  \n",
            "----------------------------------------------------------------------\n",
            "   3    |    -    |   0.019436   |  0.053169  |   99.25   |  3607.08 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   4    |   20    |   0.015612   |     -      |     -     |   32.84  \n",
            "   4    |   40    |   0.000443   |     -      |     -     |   31.58  \n",
            "   4    |   60    |   0.022142   |     -      |     -     |   31.76  \n",
            "   4    |   80    |   0.026145   |     -      |     -     |   31.42  \n",
            "   4    |   100   |   0.000209   |     -      |     -     |   31.79  \n",
            "   4    |   120   |   0.000287   |     -      |     -     |   31.92  \n",
            "   4    |   140   |   0.000134   |     -      |     -     |   31.57  \n",
            "   4    |   160   |   0.000119   |     -      |     -     |   31.89  \n",
            "   4    |   180   |   0.013992   |     -      |     -     |   31.35  \n",
            "   4    |   200   |   0.034149   |     -      |     -     |   31.55  \n",
            "   4    |   220   |   0.008139   |     -      |     -     |   31.71  \n",
            "   4    |   240   |   0.000088   |     -      |     -     |   31.79  \n",
            "   4    |   260   |   0.000121   |     -      |     -     |   31.34  \n",
            "   4    |   280   |   0.000314   |     -      |     -     |   31.33  \n",
            "   4    |   300   |   0.000091   |     -      |     -     |   31.35  \n",
            "   4    |   320   |   0.000330   |     -      |     -     |   31.49  \n",
            "   4    |   340   |   0.057022   |     -      |     -     |   31.43  \n",
            "   4    |   360   |   0.000088   |     -      |     -     |   31.48  \n",
            "   4    |   380   |   0.034741   |     -      |     -     |   31.85  \n",
            "   4    |   400   |   0.002376   |     -      |     -     |   31.63  \n",
            "   4    |   420   |   0.000359   |     -      |     -     |   31.61  \n",
            "   4    |   440   |   0.044646   |     -      |     -     |   31.39  \n",
            "   4    |   460   |   0.000891   |     -      |     -     |   31.72  \n",
            "   4    |   480   |   0.000279   |     -      |     -     |   31.52  \n",
            "   4    |   500   |   0.000124   |     -      |     -     |   31.22  \n",
            "   4    |   520   |   0.000157   |     -      |     -     |   31.62  \n",
            "   4    |   540   |   0.000094   |     -      |     -     |   31.82  \n",
            "   4    |   560   |   0.040933   |     -      |     -     |   31.73  \n",
            "   4    |   580   |   0.000394   |     -      |     -     |   31.63  \n",
            "   4    |   600   |   0.025320   |     -      |     -     |   31.44  \n",
            "   4    |   620   |   0.000156   |     -      |     -     |   31.51  \n",
            "   4    |   640   |   0.000151   |     -      |     -     |   31.21  \n",
            "   4    |   660   |   0.000096   |     -      |     -     |   31.24  \n",
            "   4    |   680   |   0.000170   |     -      |     -     |   32.13  \n",
            "   4    |   700   |   0.000071   |     -      |     -     |   31.76  \n",
            "   4    |   720   |   0.000103   |     -      |     -     |   31.84  \n",
            "   4    |   740   |   0.000111   |     -      |     -     |   31.50  \n",
            "   4    |   760   |   0.000053   |     -      |     -     |   31.64  \n",
            "   4    |   780   |   0.000085   |     -      |     -     |   31.47  \n",
            "   4    |   800   |   0.024565   |     -      |     -     |   31.38  \n",
            "   4    |   820   |   0.008187   |     -      |     -     |   31.29  \n",
            "   4    |   840   |   0.000087   |     -      |     -     |   31.69  \n",
            "   4    |   860   |   0.000188   |     -      |     -     |   31.13  \n",
            "   4    |   880   |   0.000042   |     -      |     -     |   31.31  \n",
            "   4    |   900   |   0.000071   |     -      |     -     |   31.35  \n",
            "   4    |   920   |   0.000091   |     -      |     -     |   31.62  \n",
            "   4    |   940   |   0.000260   |     -      |     -     |   31.67  \n",
            "   4    |   960   |   0.000063   |     -      |     -     |   31.26  \n",
            "   4    |   980   |   0.000030   |     -      |     -     |   31.54  \n",
            "   4    |  1000   |   0.000084   |     -      |     -     |   31.85  \n",
            "   4    |  1020   |   0.000123   |     -      |     -     |   31.86  \n",
            "   4    |  1040   |   0.000118   |     -      |     -     |   31.44  \n",
            "   4    |  1060   |   0.000086   |     -      |     -     |   31.65  \n",
            "   4    |  1080   |   0.000046   |     -      |     -     |   31.37  \n",
            "   4    |  1100   |   0.000067   |     -      |     -     |   31.64  \n",
            "   4    |  1120   |   0.052348   |     -      |     -     |   31.72  \n",
            "   4    |  1140   |   0.000052   |     -      |     -     |   31.46  \n",
            "   4    |  1160   |   0.015453   |     -      |     -     |   31.21  \n",
            "   4    |  1180   |   0.000134   |     -      |     -     |   31.73  \n",
            "   4    |  1200   |   0.000108   |     -      |     -     |   31.78  \n",
            "   4    |  1220   |   0.000035   |     -      |     -     |   32.18  \n",
            "   4    |  1240   |   0.000040   |     -      |     -     |   31.66  \n",
            "   4    |  1260   |   0.000066   |     -      |     -     |   31.93  \n",
            "   4    |  1280   |   0.000085   |     -      |     -     |   31.56  \n",
            "   4    |  1300   |   0.000085   |     -      |     -     |   31.55  \n",
            "   4    |  1320   |   0.000032   |     -      |     -     |   31.71  \n",
            "   4    |  1340   |   0.000056   |     -      |     -     |   31.59  \n",
            "   4    |  1360   |   0.000036   |     -      |     -     |   31.67  \n",
            "   4    |  1380   |   0.000138   |     -      |     -     |   31.87  \n",
            "   4    |  1400   |   0.000165   |     -      |     -     |   31.72  \n",
            "   4    |  1420   |   0.000019   |     -      |     -     |   31.64  \n",
            "   4    |  1440   |   0.000056   |     -      |     -     |   31.95  \n",
            "   4    |  1460   |   0.098827   |     -      |     -     |   31.80  \n",
            "   4    |  1480   |   0.000123   |     -      |     -     |   31.64  \n",
            "   4    |  1500   |   0.000073   |     -      |     -     |   31.63  \n",
            "   4    |  1520   |   0.000116   |     -      |     -     |   31.84  \n",
            "   4    |  1540   |   0.000100   |     -      |     -     |   31.82  \n",
            "   4    |  1560   |   0.000091   |     -      |     -     |   31.91  \n",
            "   4    |  1580   |   0.000074   |     -      |     -     |   31.66  \n",
            "   4    |  1600   |   0.000048   |     -      |     -     |   32.01  \n",
            "   4    |  1620   |   0.000064   |     -      |     -     |   31.28  \n",
            "   4    |  1640   |   0.000065   |     -      |     -     |   31.76  \n",
            "   4    |  1660   |   0.000079   |     -      |     -     |   31.67  \n",
            "   4    |  1680   |   0.000099   |     -      |     -     |   31.36  \n",
            "   4    |  1700   |   0.000107   |     -      |     -     |   31.90  \n",
            "   4    |  1720   |   0.000059   |     -      |     -     |   31.72  \n",
            "   4    |  1740   |   0.000069   |     -      |     -     |   32.03  \n",
            "   4    |  1760   |   0.000071   |     -      |     -     |   31.66  \n",
            "   4    |  1780   |   0.000305   |     -      |     -     |   31.81  \n",
            "   4    |  1800   |   0.000061   |     -      |     -     |   31.75  \n",
            "   4    |  1820   |   0.000123   |     -      |     -     |   31.47  \n",
            "   4    |  1840   |   0.000030   |     -      |     -     |   32.08  \n",
            "   4    |  1860   |   0.028130   |     -      |     -     |   31.51  \n",
            "   4    |  1880   |   0.000132   |     -      |     -     |   31.70  \n",
            "   4    |  1900   |   0.000037   |     -      |     -     |   31.66  \n",
            "   4    |  1920   |   0.000084   |     -      |     -     |   32.17  \n",
            "   4    |  1940   |   0.000098   |     -      |     -     |   31.83  \n",
            "   4    |  1960   |   0.000030   |     -      |     -     |   31.92  \n",
            "   4    |  1980   |   0.079758   |     -      |     -     |   32.25  \n",
            "   4    |  2000   |   0.000329   |     -      |     -     |   31.46  \n",
            "   4    |  2020   |   0.000050   |     -      |     -     |   31.92  \n",
            "   4    |  2040   |   0.070507   |     -      |     -     |   31.56  \n",
            "   4    |  2060   |   0.031618   |     -      |     -     |   31.85  \n",
            "   4    |  2068   |   0.000048   |     -      |     -     |   12.76  \n",
            "----------------------------------------------------------------------\n",
            "   4    |    -    |   0.007210   |  0.038041  |   99.64   |  3614.75 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   5    |   20    |   0.000035   |     -      |     -     |   33.22  \n",
            "   5    |   40    |   0.000064   |     -      |     -     |   31.50  \n",
            "   5    |   60    |   0.000049   |     -      |     -     |   32.06  \n",
            "   5    |   80    |   0.000052   |     -      |     -     |   32.10  \n",
            "   5    |   100   |   0.000089   |     -      |     -     |   32.67  \n",
            "   5    |   120   |   0.000124   |     -      |     -     |   32.13  \n",
            "   5    |   140   |   0.000054   |     -      |     -     |   31.35  \n",
            "   5    |   160   |   0.000045   |     -      |     -     |   31.85  \n",
            "   5    |   180   |   0.000054   |     -      |     -     |   31.75  \n",
            "   5    |   200   |   0.000624   |     -      |     -     |   31.39  \n",
            "   5    |   220   |   0.000097   |     -      |     -     |   31.73  \n",
            "   5    |   240   |   0.000043   |     -      |     -     |   32.12  \n",
            "   5    |   260   |   0.000056   |     -      |     -     |   31.67  \n",
            "   5    |   280   |   0.000042   |     -      |     -     |   31.75  \n",
            "   5    |   300   |   0.000170   |     -      |     -     |   31.41  \n",
            "   5    |   320   |   0.000082   |     -      |     -     |   31.80  \n",
            "   5    |   340   |   0.000415   |     -      |     -     |   31.33  \n",
            "   5    |   360   |   0.000026   |     -      |     -     |   31.38  \n",
            "   5    |   380   |   0.000044   |     -      |     -     |   31.24  \n",
            "   5    |   400   |   0.000096   |     -      |     -     |   31.57  \n",
            "   5    |   420   |   0.000044   |     -      |     -     |   31.66  \n",
            "   5    |   440   |   0.000183   |     -      |     -     |   31.14  \n",
            "   5    |   460   |   0.000042   |     -      |     -     |   31.36  \n",
            "   5    |   480   |   0.000088   |     -      |     -     |   31.79  \n",
            "   5    |   500   |   0.000051   |     -      |     -     |   31.68  \n",
            "   5    |   520   |   0.000134   |     -      |     -     |   32.05  \n",
            "   5    |   540   |   0.000067   |     -      |     -     |   31.19  \n",
            "   5    |   560   |   0.000080   |     -      |     -     |   31.57  \n",
            "   5    |   580   |   0.000072   |     -      |     -     |   31.71  \n",
            "   5    |   600   |   0.000055   |     -      |     -     |   31.36  \n",
            "   5    |   620   |   0.000037   |     -      |     -     |   31.53  \n",
            "   5    |   640   |   0.000075   |     -      |     -     |   31.50  \n",
            "   5    |   660   |   0.000069   |     -      |     -     |   31.38  \n",
            "   5    |   680   |   0.000054   |     -      |     -     |   31.59  \n",
            "   5    |   700   |   0.000067   |     -      |     -     |   31.69  \n",
            "   5    |   720   |   0.000018   |     -      |     -     |   31.71  \n",
            "   5    |   740   |   0.000056   |     -      |     -     |   31.75  \n",
            "   5    |   760   |   0.000032   |     -      |     -     |   31.52  \n",
            "   5    |   780   |   0.000022   |     -      |     -     |   31.41  \n",
            "   5    |   800   |   0.000019   |     -      |     -     |   31.43  \n",
            "   5    |   820   |   0.000109   |     -      |     -     |   31.20  \n",
            "   5    |   840   |   0.000070   |     -      |     -     |   31.47  \n",
            "   5    |   860   |   0.000379   |     -      |     -     |   31.10  \n",
            "   5    |   880   |   0.000185   |     -      |     -     |   31.46  \n",
            "   5    |   900   |   0.000095   |     -      |     -     |   31.73  \n",
            "   5    |   920   |   0.000027   |     -      |     -     |   31.60  \n",
            "   5    |   940   |   0.000014   |     -      |     -     |   31.36  \n",
            "   5    |   960   |   0.000084   |     -      |     -     |   31.58  \n",
            "   5    |   980   |   0.000060   |     -      |     -     |   31.13  \n",
            "   5    |  1000   |   0.000085   |     -      |     -     |   31.72  \n",
            "   5    |  1020   |   0.000037   |     -      |     -     |   31.74  \n",
            "   5    |  1040   |   0.000048   |     -      |     -     |   31.62  \n",
            "   5    |  1060   |   0.000036   |     -      |     -     |   32.03  \n",
            "   5    |  1080   |   0.000174   |     -      |     -     |   31.75  \n",
            "   5    |  1100   |   0.000038   |     -      |     -     |   31.68  \n",
            "   5    |  1120   |   0.000024   |     -      |     -     |   31.66  \n",
            "   5    |  1140   |   0.000570   |     -      |     -     |   31.86  \n",
            "   5    |  1160   |   0.000026   |     -      |     -     |   31.34  \n",
            "   5    |  1180   |   0.000320   |     -      |     -     |   31.49  \n",
            "   5    |  1200   |   0.000021   |     -      |     -     |   30.97  \n",
            "   5    |  1220   |   0.000044   |     -      |     -     |   31.46  \n",
            "   5    |  1240   |   0.000105   |     -      |     -     |   31.25  \n",
            "   5    |  1260   |   0.000013   |     -      |     -     |   31.42  \n",
            "   5    |  1280   |   0.000103   |     -      |     -     |   32.29  \n",
            "   5    |  1300   |   0.000057   |     -      |     -     |   32.19  \n",
            "   5    |  1320   |   0.000025   |     -      |     -     |   31.55  \n",
            "   5    |  1340   |   0.000093   |     -      |     -     |   31.74  \n",
            "   5    |  1360   |   0.000028   |     -      |     -     |   31.53  \n",
            "   5    |  1380   |   0.075028   |     -      |     -     |   31.63  \n",
            "   5    |  1400   |   0.000029   |     -      |     -     |   31.57  \n",
            "   5    |  1420   |   0.000075   |     -      |     -     |   31.04  \n",
            "   5    |  1440   |   0.000022   |     -      |     -     |   31.99  \n",
            "   5    |  1460   |   0.000040   |     -      |     -     |   31.83  \n",
            "   5    |  1480   |   0.000026   |     -      |     -     |   31.77  \n",
            "   5    |  1500   |   0.000020   |     -      |     -     |   31.81  \n",
            "   5    |  1520   |   0.000146   |     -      |     -     |   31.41  \n",
            "   5    |  1540   |   0.000012   |     -      |     -     |   31.03  \n",
            "   5    |  1560   |   0.000019   |     -      |     -     |   31.39  \n",
            "   5    |  1580   |   0.000130   |     -      |     -     |   31.57  \n",
            "   5    |  1600   |   0.000015   |     -      |     -     |   31.55  \n",
            "   5    |  1620   |   0.000056   |     -      |     -     |   31.76  \n",
            "   5    |  1640   |   0.000063   |     -      |     -     |   31.69  \n",
            "   5    |  1660   |   0.000150   |     -      |     -     |   31.75  \n",
            "   5    |  1680   |   0.000019   |     -      |     -     |   31.96  \n",
            "   5    |  1700   |   0.000045   |     -      |     -     |   31.52  \n",
            "   5    |  1720   |   0.000031   |     -      |     -     |   31.46  \n",
            "   5    |  1740   |   0.000223   |     -      |     -     |   31.80  \n",
            "   5    |  1760   |   0.000234   |     -      |     -     |   31.34  \n",
            "   5    |  1780   |   0.000072   |     -      |     -     |   31.63  \n",
            "   5    |  1800   |   0.000027   |     -      |     -     |   31.34  \n",
            "   5    |  1820   |   0.000017   |     -      |     -     |   31.43  \n",
            "   5    |  1840   |   0.000031   |     -      |     -     |   32.18  \n",
            "   5    |  1860   |   0.000050   |     -      |     -     |   31.64  \n",
            "   5    |  1880   |   0.000041   |     -      |     -     |   31.48  \n",
            "   5    |  1900   |   0.000027   |     -      |     -     |   31.84  \n",
            "   5    |  1920   |   0.000111   |     -      |     -     |   31.27  \n",
            "   5    |  1940   |   0.000335   |     -      |     -     |   31.39  \n",
            "   5    |  1960   |   0.000037   |     -      |     -     |   31.39  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40d1ac07-16aa-4d8a-88ab-50095e757209"
      },
      "source": [
        ""
      ],
      "id": "40d1ac07-16aa-4d8a-88ab-50095e757209",
      "execution_count": null,
      "outputs": []
    }
  ]
}